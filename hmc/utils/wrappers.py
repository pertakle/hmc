import numpy as np
import torch
from hmc.agents.nn.noisy_linear import NoisyLinear
import gymnasium as gym

class TorchToNumpyWrapperVec(gym.vector.VectorWrapper):
    def __init__(self, env: gym.vector.VectorEnv, device=None) -> None:
        super().__init__(env)
        self._device = device

    def reset(self, *args, **kwargs):
        next_obs, info = self.env.reset(*args, **kwargs)
        return self.torch_to_numpy(next_obs, np.long), info

    def step(self, actions: np.ndarray):
        next_obs, rewards, terminations, truncations, info = self.env.step(
            self.numpy_to_torch(actions, torch.long)
        )
        return (
            self.torch_to_numpy(next_obs, np.long),
            self.torch_to_numpy(rewards, np.float32),
            self.torch_to_numpy(terminations, np.bool),
            self.torch_to_numpy(truncations, np.bool),
            info,
        )

    def numpy_to_torch(self, array: np.ndarray, dtype=None) -> torch.Tensor:
        return torch.as_tensor(array, dtype=dtype, device=self._device)

    def torch_to_numpy(self, tensor: torch.Tensor, dtype=None) -> np.ndarray:
        return tensor.cpu().numpy()


class PositiveWrapperVec(gym.vector.VectorWrapper):
    def __init__(self, env: gym.vector.VectorEnv):
        super().__init__(env)

    def step(self, actions):
        next_obs, rewards, terminations, truncations, info = super().step(actions)
        return next_obs, rewards + 1 + terminations, terminations, truncations, info


def typed_torch_function(device, *types, via_np=False):
    """Typed Torch function decorator.

    The positional input arguments are converted to torch Tensors of the given
    types and on the given device; for NumPy arrays on the same device,
    the conversion should not copy the data.

    The torch Tensors generated by the wrapped function are converted back
    to Numpy arrays before returning (while keeping original tuples, lists,
    and dictionaries).
    """
    import torch

    def check_typed_torch_function(wrapped, args):
        if len(types) != len(args):
            while hasattr(wrapped, "__wrapped__"):
                wrapped = wrapped.__wrapped__
            raise AssertionError(
                "The typed_torch_function decorator for {} expected {} arguments, but got {}".format(
                    wrapped, len(types), len(args)
                )
            )

    def structural_map(value):
        if isinstance(value, torch.Tensor):
            return value.numpy(force=True)
        if isinstance(value, tuple):
            return tuple(structural_map(element) for element in value)
        if isinstance(value, list):
            return [structural_map(element) for element in value]
        if isinstance(value, dict):
            return {key: structural_map(element) for key, element in value.items()}
        return value

    class TypedTorchFunctionWrapper:
        def __init__(self, func):
            self.__wrapped__ = func

        def __call__(self, *args, **kwargs):
            check_typed_torch_function(self.__wrapped__, args)
            return structural_map(
                self.__wrapped__(
                    *[
                        torch.as_tensor(
                            np.asarray(arg) if via_np else arg, dtype=typ, device=device
                        )
                        for arg, typ in zip(args, types)
                    ],
                    **kwargs
                )
            )

        def __get__(self, instance, cls):
            return TypedTorchFunctionWrapper(self.__wrapped__.__get__(instance, cls))

    return TypedTorchFunctionWrapper


def torch_init_with_orthogonal_and_zeros(module):
    """Initialize weights of a PyTorch module with Xavier and zeros initializers."""
    import torch

    if isinstance(
        module,
        (
            NoisyLinear,
            torch.nn.Linear,
            torch.nn.Conv1d,
            torch.nn.Conv2d,
            torch.nn.Conv3d,
            torch.nn.ConvTranspose1d,
            torch.nn.ConvTranspose2d,
            torch.nn.ConvTranspose3d,
        ),
    ):
        torch.nn.init.orthogonal_(module.weight)
        if module.bias is not None:
            torch.nn.init.zeros_(module.bias)

def torch_init_with_xavier_and_zeros(module):
    """Initialize weights of a PyTorch module with Xavier and zeros initializers."""
    import torch

    if isinstance(
        module,
        (
            NoisyLinear,
            torch.nn.Linear,
            torch.nn.Conv1d,
            torch.nn.Conv2d,
            torch.nn.Conv3d,
            torch.nn.ConvTranspose1d,
            torch.nn.ConvTranspose2d,
            torch.nn.ConvTranspose3d,
        ),
    ):
        torch.nn.init.xavier_uniform_(module.weight)
        if module.bias is not None:
            torch.nn.init.zeros_(module.bias)
